# Classificazione mediante tecniche di Deep Learning

Per la parte di "Representation Learning" abbiamo preso ispirazione dal paper pubblicato nel 2020 "Deep Fusion Siamese Network for Automatic Kinship Verification" [@siamese].
La soluzione proposta utilizza una rete siamese in cui si estraggono *feature* dalle immagini in input, si combinano mediante operazioni di fusione per ricavare una metrica di similarità tra di esse. L'uso di questa architettura ci è stato suggerito dalla soluzione del primo classificato della competizione su cui si basa questo progetto.

![Generica architettura di una rete siamese come mostrata nell'articolo di Yu, Li, Hao e Xie](https://drive.google.com/uc?id=1fAUq-R9c2-7XN4R0Q-T1LsF0vSjvCaJh){width=100%}

La rete si compone di una parte di *feature extraction* in cui vengono estratte delle *feature* dalle immagini in input utilizzando dei modelli già addestrati, cioè delle reti *backbone*. Da ogni immagine viene estratto un vettore di *feature* il quale viene fuso con il suo corrispondente attraverso diverse operazioni. I vettori fusione risultanti vengono concatenati in un unico vettore che farà da input per i livelli *fully connected* successivi, fino ad arrivare all'ultimo neurone che, grazie alla funzione "sigmoide", restituisce la probabilità che i due volti appartengano alla classe "parenti".

La nostra versione utilizza l'architettura base proposta nel paper introducendo delle variazioni volte a migliorare le prestazioni in termini di accuratezza, prendendo spunto da quella del primo classificato nella competizione.
Si descrive di seguito l'architettura proposta.

1. Viene definita una funzione `data_augmentation` che prende un'immagine come parametro e vi applica diverse trasformazioni di *data augmentation*. Le trasformazioni includono rotazione, zoom, traslazione, ribaltamento, variazione del contrasto e luminosità in modo casuale. L'output finale di questa funzione è l'immagine sottoposta a tutte queste trasformazioni. Questo costringe la rete ad essere robusta nei confronti delle variazioni indicate precedentemente e inoltre permette di aumentare il numero di istanze del *dataset* fornito in input, dato che sono in numero ridotto. I valori utilizzati nei vari *layer* di *processing* permettono di variare in maniera non trascurabile le immagini di input senza però introdurre variazioni troppo elevate;

2. Vengono generati gli *embedding* per le immagini utilizzando due *backbone* diverse per ciascuna immagine, che abbiamo scelto essere "ResNet-50" e "SeNet-50";

3. Ogni *embedding* viene appiattito mediante un'operazione di "flattening";

4. Come funzioni di fusione delle *feature* vengono utilizzate le stesse che sono state indicate nella sezione di classificazione tradizionale. Le operazioni includono la somma, la moltiplicazione, la sottrazione, la somma dei quadrati delle componenti, la differenza dei quadrati delle componenti, il quadrato della somma e della differenza delle componenti. Si ottengono così 2 risultati per operazione.

5. Le fusioni vengono sottoposte a un livello *fully connected* di 256 neuroni. Questo serve per ridurre le dimensioni di ogni vettore ottenuto dalla fusione ed introduce anche *layer* addestrabili;

6. I vettori vengono concatenati orizzontalmente formandone uno unico;

7. Il vettore concatenato viene passato attraverso due livelli densi, con 512 e 64 neuroni rispettivamente. Ogni *layer* viene creato con la funzione `dense_layer` che crea un livello denso con il numero specificato di neuroni. Ogni livello ha come funzione di attivazione "ReLU". A questo livello denso seguono un livello di *batch normalization* e uno di *dropout* con una probabilità di 0.1;

8. L'output del secondo livello denso viene passato attraverso il livello di output finale. Se il problema è multiclasse viene utilizzata una funzione di attivazione "softmax" e l'output avrà un numero di neuroni uguale al numero di classi. Altrimenti, viene utilizzata una funzione di attivazione "sigmoide" e l'output sarà un singolo neurone;

9. Viene compilato il modello con una funzione di *loss* appropriata per il tipo di problema, "binary crossentropy" nel caso binario o "categorical crossentropy" nel caso multiclasse, e con Adam come ottimizzatore, usando un *learning rate* molto basso, pari a 0.00001. Questo serve per evitare il fenomeno di "Catastrophic Forgetting" e preservare ciò che la rete ha precedentemente appreso.

![Architettura della rete siamese utilizzata in questo progetto](https://drive.google.com/uc?id=1V12rm1BgcWzvLtLqwX7_td-fdQ1Ycl7y){width=100%}

Per il dataset viene creata una classe ad hoc, `FaceDataset`, che estende la classe `Sequence` di Keras. La classe "FaceDataset" è progettata per generare *batch* di dati ed etichette da utilizzare durante l'addestramento. Questa soluzione è stata obbligata dal fatto che le immagini prima di essere date in input alla rete devono essere preprocessate convertendole da un formato `int.8` a `float.32`. Questa conversione causa una saturazione rapida della RAM. Caricando e processando le immagini un *batch* alla volta si evita questa problematica.

I modelli utilizzati come *backbone* della rete sono "ResNet-50" e "SeNet-50" addestrati sul _dataset_ "VGGFace2" [@vggface2]. Il _dataset_ contiene 3.31 milioni di immagini di 9131 soggetti diversi, con una media di 362.6 immagini per soggetto. Le immagini derivano da "Google Image Search" e hanno una grande varietà di pose, età, illuminazioni, etnie e professioni (e.g. attori, atleti, politici).
Di queste reti si utilizza solamente la parte convoluzionale. Inoltre, svolgendo diversi test abbiamo ottenuto migliori risultati impostando tutti i _layer_ delle reti come addestrabili. Le due reti accettano immagini di dimensione 224x224x3, quindi le immagini del *dataset* non necessitano di un ridimensionamento.

Come *validation set* si è deciso di utilizzare tutte le relazioni rappresentate da immagini appartenenti alle famiglie il cui nome inizia per `F09`, come nella soluzione realizzata dal vincitore della _challenge_. La suddivisione è 84% _training_, 16% _validation_. Poi si utilizzano i `DataFrame` istanziati per creare con la classe `FaceDataset` delle strutture dati processabili da "Keras", dove le istanze di _training_ vengono mescolate.

Vengono anche utilizzate diverse *callback* per l'addestramento:

1. `EarlyStopping` serve per fermare preventivamente l'addestramento in caso non ci fosse un miglioramento dell'accuratezza sul *validation set* per un numero di epoche specificato dal parametro `patience`, in questo caso posto a 10;
2. `ModelCheckpoint` salva il modello se quest'ultimo ottiene uno *score* sul *validation* maggiore del precedente salvato;
3. `ReduceLROnPlateau` riduce il learning rate di un fattore dato dal parametro `factor`, pari a 0.1, dopo un numero `patience` di epoche, pari a 5, se non ci sono stati miglioramenti nell'accuratezza sul *validation set*.

Una delle prime cose che è possibile notare dai risultati è il deciso miglioramento rispetto ai modelli di XGBoost. Lo *score* sul *test set* è infatti del 63%, rispetto al 53% della sua controparte tradizionale, segno che in questo caso la rete è stata capace di apprendere delle caratteristiche salienti. In più, osservando l'accuratezza sul *train set*, che è del 75%, notiamo come l'*overfitting* sia rimasto decisamente contenuto. Questi ottimi risultati sono confermati infatti dalla *confusion matrix*, che mostra sulla diagonale un numero molto elevato di veri positivi e veri negativi. Si può anche osservare come, nonostante la classe dei negativi nel *training set* fosse la più numerosa, il modello ha imparato a predire nella maggior parte dei casi un'istanza come della classe dei positivi, segno che non sta "tirando a caso" avendo imparando la distribuzione dei *pattern*.

```{=latex}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/net_bin_cm.png}
    \caption{\textit{Confusion matrix} per la rete siamese. L'accuratezza di \textit{training} era 75\%, quella di \textit{test} 63\%}
\end{figure}
```

Si sono poi analizzati quei casi in cui la rete ha dato i migliori risultati in assoluto, secondo l'errore, in ordine decrescente. Come si può notare, la rete è stata in grado di considerare tutte le *feature* facciali presenti nelle immagini. Infatti, negli esempi mostrati si può vedere come persone dal profilo asiatico, con occhi a mandorla e nasi più schiacciati, non vengano mai associate con legami di parentela a persone dal profilo caucasico, nemmeno quando queste potrebbero avere alcune caratteristiche simili, come gli occhi più stretti. Più in generale, la rete è stata capace di associare persone che possiedono volti simili tra di loro, come ad esempio persone che hanno le stesse sopracciglia, lo stesso naso, la stessa forma della bocca, anche quando queste si trovano in pose differenti: un volto può sorridere e l'altro no, ma questo non pregiudica il riconoscimento del legame di parentela. Infine, la rete è stata capace di trovare legami di parentela anche utilizzando immagini in bianco e nero, prescindendo dalle *feature* di colore.

```{=latex}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{images/top_net_bin_1.png}
    \includegraphics[width=0.45\textwidth]{images/top_net_bin_2.png}
    \includegraphics[width=0.45\textwidth]{images/top_net_bin_3.png}
    \includegraphics[width=0.45\textwidth]{images/top_net_bin_4.png}
    \includegraphics[width=0.45\textwidth]{images/top_net_bin_5.png}
    \caption{Le cinque istanze in cui la rete siamese ha dato i migliori risultati}
\end{figure}
```

Si osservano quindi i casi in cui la rete ha dato i peggiori risultati in assoluto in ordine decrescente. Dai risultati si può osservare come le caratteristiche che hanno ingannato il classificatore sono state la presenza di occhiali da sole o da vista, volti con smorfie, volti che hanno caratteristiche molto simili tra loro in termini di naso, sopracciglia, labbra, bocca. Un'ultima caratteristica che si può riscontrare nelle immagini che rende difficile la classificazione è la bassa risoluzione delle stesse. È evidente come molte istanze siano difficilmente classificabili anche per un essere umano.

```{=latex}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{images/last_net_bin_1.png}
    \includegraphics[width=0.45\textwidth]{images/last_net_bin_2.png}
    \includegraphics[width=0.45\textwidth]{images/last_net_bin_3.png}
    \includegraphics[width=0.45\textwidth]{images/last_net_bin_4.png}
    \includegraphics[width=0.45\textwidth]{images/last_net_bin_5.png}
    \caption{Le cinque istanze in cui la rete siamese ha dato i peggiori risultati}
\end{figure}
```

Quando si è trattato di addestrare la rete per risolvere il problema multiclasse, il procedimento è stato in tutto e per tutto analogo a quello già visto. Anche la struttura della rete è analoga, sostituendo unicamente l'ultimo *layer* che, anziché avere un solo neurone, ne possiede uno per classe, cioè 11. Inoltre, la funzione di attivazione di quel livello viene sostituita con una "softmax" anziché "sigmoid" e l'errore diventa "categorical crossentropy". Infine, in questo caso vengono calcolati i pesi di ciascuna classe in base alla loro frequenza nel *training set*: alle classi le cui istanze hanno frequenza maggiore viene attribuito un peso minore. Questi saranno poi utilizzati durante l'addestramento.

Una delle prime cose che è possibile notare dai risultati dell'addestramento è il deciso miglioramento rispetto ai modelli di "XGBoost". Lo *score* sul *test set* è infatti del 61%, rispetto al 20% della sua controparte tradizionale, segno che in questo caso la rete è stata capace di apprendere delle caratteristiche salienti. In più, osservando l'accuratezza sul *train set*, che è del 81%, notiamo come l'*overfitting* sia rimasto più contenuto. Questi risultati sono confermati infatti dalla *confusion matrix*, che mostra sulla diagonale un numero elevato di istanze classificate correttamente. Si nota come il modello riesca a classificare correttamente anche una piccola porzione delle relazioni con dei nonni, cosa che la soluzione tradizionale non era riuscita a fare. L'errore più frequente è quello commesso classificando le istanze di "fratelli" come istanze della classe "padre-figlio", probabilmente dovuto alla maggioranza del genere maschile nel *dataset* e ad una somiglianza tra queste due classi. Altri errori comuni sono quelli che vedono scambiate le classi "madre-figlio" con quella "madre-figlia" e quelli che vedono scambiate la classe "padre-figlio" con quella "padre-figlia". Questo sospettiamo sia dovuto alla all'aspetto androgino dei bambini in tenera età. Altri errori meno significativi coinvolgono la difficoltà del modello nello stabilire le relazioni di età di genere per alcune classi.

```{=latex}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/net_mul_cm.png}
    \caption{\textit{Confusion matrix} per la rete siamese. L'accuratezza di \textit{training} era 82\%, quella di \textit{test} 61\%}
\end{figure}
```

Si osservano quei casi in cui la rete ha dato i migliori risultati in assoluto, secondo l'errore, in ordine decrescente. Come si può notare, la rete è stata in grado di considerare tutte le *feature* facciali presenti nelle immagini, in quanto riesce a funzionare correttamente anche in caso di occlusioni parziali del volto. Più in generale, la rete è stata capace di associare persone che possiedono volti simili tra di loro, come ad esempio persone che hanno le stesse sopracciglia, lo stesso naso, la stessa forma della bocca, anche quando queste si trovano in pose differenti: un volto può sorridere e l'altro no, ma questo non pregiudica il riconoscimento del legame di parentela. Infine, la rete è stata capace di trovare legami di parentela anche utilizzando immagini in bianco e nero o in bassa risoluzione, prescindendo dalle *feature* di colore o dai dettagli.

```{=latex}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{images/top_net_mul_1.png}
    \includegraphics[width=0.45\textwidth]{images/top_net_mul_2.png}
    \includegraphics[width=0.45\textwidth]{images/top_net_mul_3.png}
    \includegraphics[width=0.45\textwidth]{images/top_net_mul_4.png}
    \includegraphics[width=0.45\textwidth]{images/top_net_mul_5.png}
    \caption{Le cinque istanze in cui la rete siamese ha dato i migliori risultati}
\end{figure}
```

Si osservano infine quei casi in cui la rete ha dato i peggiori risultati in assoluto, secondo l'errore, in ordine decrescente. In questo caso si può notare come la rete abbia sbagliato su istanze "*borderline*", cioè immagini particolarmente in bassa risoluzione e anche in bianco e nero. Gli errori riscontrati sono in accordo con i risultati che emergono dalla *confusion matrix*. Talvolta la rete individua una classe simile a quella reale, ma sbaglia le relazioni di età tra i volti o il genere della persona.

```{=latex}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{images/last_net_mul_1.png}
    \includegraphics[width=0.45\textwidth]{images/last_net_mul_2.png}
    \includegraphics[width=0.45\textwidth]{images/last_net_mul_3.png}
    \includegraphics[width=0.45\textwidth]{images/last_net_mul_4.png}
    \includegraphics[width=0.45\textwidth]{images/last_net_mul_5.png}
    \caption{Le cinque istanze in cui la rete siamese ha dato i peggiori risultati}
\end{figure}
```
