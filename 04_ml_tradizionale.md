# Classificazione tradizionale

## Preparazione dei dataset

Una volta determinate le *feature* da estrarre, non si è fatto altro che effettuare la loro elaborazione per ogni immagine a colori corretta, indipendentemente dal fatto che quest'ultima appartenesse al *training set* o al *test set*. Nel caso in cui da un'immagine non fosse possibile estrarre una o più *feature*, questa viene interamente scartata.

Una volta estratte le *feature*, è stato necessario normalizzarle. I descrittori che sono realizzati come istogrammi non possono però esserlo, in quanto ciascuna componente ha un significato che è legato a quello delle altre. Riscalare i valori presi singolarmente altererebbe i rapporti tra questi, facendo perdere parte del significato che portano con sé. Tanto più che la normalizzazione degli istogrammi viene già fatta, semplicemente dividendo il valore di ciascun "bin" per la somma dei valori, dando origine ad una funzione di distribuzione di probabilità discreta.

Discorso analogo vale per i descrittori di Fourier ellittici: sono coefficienti che codificano informazioni sullo "spettro delle frequenze" di una forma in termini di ellissi la cui somma restituisce la silhouette originale. Non sono perciò importanti solamente i rapporti tra questi, ma anche il loro valore assoluto, che indica con che ampiezza una data frequenza è presente nel segnale di partenza. Si ricorda inoltre che la loro estrazione già prevede una fase di normalizzazione degli stessi.

I descrittori che non ricadono in queste categorie sono invece i centroidi, che sono stati resi indipendenti dalle dimensioni dell'immagine da cui sono stati estratti, ma non sono normalizzati. Analogamente non sono normalizzati l'elongazione e la convessità, così come i descrittori del tono della pelle del volto, che sono distanze, quindi possono variare tra $0$ e $+\infty$.

Per questo motivo, tutti questi descrittori sono stati normalizzati mediante "standard scaling", ovvero tramite la sottrazione della media dei valori e divisione per la deviazione standard. In questo modo, la distribuzione dei valori di ciascun descrittore, se può essere espressa mediante una funzione gaussiana, è una gaussiana di media 0 e varianza 1.

## Addestramento del classificatore per il primo problema

Una volta normalizzate le *feature*, si è trattato di sostituire le *feature* delle immagini alle stesse nel *dataset* di *training* delle coppie del primo problema. Viene quindi costruito un nuovo *dataset* che ha tante righe quante sono le istanze dalle quali è stato possibile estrarre tutte le *feature* e un numero di colonne pari al doppio delle *feature* per ciascuna immagine. Questo perché le *feature* estratte da ciascuna delle due immagini della coppia vengono affiancate in questo nuovo *dataset*. Nel *dataset* originale si trattengono solamente le coppie di immagini per cui le *feature* sono state estratte da entrambe, così da avere le etichette delle classi. La stessa cosa viene fatta anche per il *dataset* di *test* delle coppie per il primo problema.

Le operazioni di estrazione variano leggermente le percentuali sul totale delle due classi, sia per il *training set* che per il *test set*. Ciò ci permette di non effettuare ulteriori operazioni di aggiustamento delle istanze, ma verranno considerati i diversi pesi di ciascuna classe durante l'addestramento.

Le funzioni che possono essere utilizzate per combinare le *feature* delle coppie di immagini sono:

* somma: $x + y$
* prodotto: $x \cdot y$
* differenza: $x-y$
* somma al quadrato: $(x+y)^2$
* differenza al quadrato: $(x-y)^2$
* somma dei quadrati: $x^2 + y^2$
* differenza dei quadrati: $x^2 - y^2$

Queste funzioni vogliono essere delle versioni semplificate di metriche di distanza o di similarità. In generale, somme o prodotti "_element-wise_" restituiscono valori tanto più elevati quanto più gli operandi sono vicini tra loro, le differenze valori tanto più grandi quanto più gli operandi sono diversi tra loro. Si è voluto evitare l'utilizzo dell'operazione di divisione, che non è definita nel caso di divisione per 0, così come la moltiplicazione o la somma di costanti, che avrebbe solamente modificato lo *scaling* delle *features*. L'uso di queste funzioni è stato ispirato dal vincitore della competizione, che utilizzava alcune di esse per combinare le *feature* estratte dalle diverse reti neurali. Infatti, questo ha permesso lui di ottenere modelli sufficientemente indipendenti tra di loro, per poterli utilizzare come "*ensemble*".

Una volta definiti i *dataset* da utilizzare e le funzioni, è stato effettuato l'addestramento vero e proprio per ottenere i migliori modelli. La famiglia di classificatori utilizzati è "XGBoost", in quanto capace di poter addestrare concorrentemente i *decision tree* che fanno parte degli "*ensemble*" che costruiscono sulla GPU. Questo rende possibile effettuare efficientemente il loro addestramento su *cluster* dotati di molte risorse di GPU.

È necessario specificare la funzione obiettivo, che altro non è che una regressione logistica che permette di effettuare classificazione. Il problema è a due sole classi, perciò è una "logistic regression" binaria. La metrica scelta per valutare l'errore del modello sul *validation set* è il più semplice tasso di errore, ovvero $\frac{FP + FN}{P + N}$.

È stata effettuata una "*grid search*" per ottenere i migliori iperparametri in corrispondenza dell'applicazione di ciascuna delle funzioni per l'aggregazione delle *feature* corrispondenti. I valori che sono stati lasciati qui indicati sono stati quelli che si sono rivelati migliori dopo diversi tentativi su differenti intervalli. Nella *grid search* gli iperparametri che si sono rivelati più significativi sono stati:

* "min_child_weight": il peso minimo che deve essere associato ad una foglia di un albero costruito, valori più grandi portano ad una terminazione maggiormente precoce del processo di costruzione-suddivisione di ogni singolo albero;
* "gamma": minima riduzione della funzione di *loss* definita perché un'ulteriore suddivisione di una foglia possa essere considerata;
* "subsample": percentuale del *training set* da campionare per la costruzione degli alberi, fatto per ridurre il potenziale *overfitting*;
* "colsample_bytree": la percentuale di colonne del *training set* da campionare durante la costruzione degli alberi, per ogni nuovo albero costruito;
* "max_depth": la profondità massima di un singolo albero, regola la complessità dello stesso e con essa la possibilità di *overfitting*;
* "learning_rate": parametro moltiplicativo che ha per scopo ridurre mano a mano il peso dei risultati degli alberi costruiti, in modo tale da evitare "salti" troppo grandi nella ricerca dei parametri ottimali del modello;
* "n_estimators": il numero massimo di alberi che verranno addestrati da XGBoost.

Il vero e proprio "*estimator*" addestrato durante la *grid search* è stato costruito mediante una "Pipeline" che applica in sequenza la funzione di aggregazione dei dati scelta e in seguito il modello da addestrare. Questo implica quindi che viene costruita una "Pipeline" per ognuna delle possibili funzioni.

La tecnica di *grid search* impiegata è la cosiddetta "Bayesian Optimization", ovvero una tecnica iterativa che si basa sulla formula di Bayes per trovare i massimi di una funzione sconosciuta a partire da un insieme di campionamenti limitato perché "costosi" da calcolare.
L'obiettivo è quello di individuare un insieme di valori, un insieme di input, su un dominio limitato da usare come "sonda". Occorre tenere conto sia di dove si ha maggiore incertezza sulla forma, sull'output, della funzione, sia di dove ci si aspetta che siano presenti i massimi della funzione, usando le informazioni già accumulate ed euristiche adeguate. Ecco quindi perché è coinvolto il teorema di Bayes, che infatti permette di calcolare la probabilità condizionata di un evento rispetto ad un secondo già accaduto, a priori, conoscendo la probabilità che quello già accaduto si verifichi.

Nel nostro caso, la funzione di cui si sta cercando di individuare il massimo è il modello che si sta addestrando che prende per input vettori nello spazio degli iperparametri e come output restituisce l'accuratezza sul *training set*, che deve essere massima. La tecnica non farà altro che campionare il modello in corrispondenza dei punti per lui sensati alla ricerca del massimo.

Si è osservato come l'uso di questa tecnica, in corrispondenza dei parametri scelti, abbia accelerato l'effettuazione della *grid search*. È stato infatti specificato di tentare 100 vettori nello spazio degli iperparametri, testandone 4 concorrentemente alla volta. La valutazione delle prestazioni degli iperparametri è stata effettuata considerando l'accuratezza come *score* e facendo "stratified k-fold cross validation". Questo significa che per ogni combinazione degli iperparametri il *training set* è stato suddiviso in tre parti: due sono state utilizzate per l'addestramento vero e proprio e una per la valutazione, cambiandole ogni volta in modo tale da poter ottenere alla fine tre *score* su cui trarre un giudizio. Il fatto che la *cross validation* fosse "stratified" ha permesso di mantenere la stessa proporzione tra le due classi in ciascuno dei *fold*, cioè delle partizioni.

I risultati dell'addestramento dei modelli che fanno uso della funzione somma ci dicono che i migliori 5 modelli hanno uno *score* medio sugli *split* che non si discosta molto dal 64,5%. L'addestramento ha prediletto un *ensemble* molto numeroso, con un numero di alberi nell'ordine del migliaio, relativamente profondi, tutti con una profondità massima pari al valore più alto scelto. Il *learning rate* scelto è stato molto basso, segno che è stato preferito un approccio conservativo. L'addestramento ha preferito non scartare una percentuale di colonne e di istanze del *dataset* di *training* molto elevata in corrispondenza di ogni nuovo albero, segno che non è stato individuato particolare *overfitting*. Da ultimo, anche il valore del parametro "gamma" rimane particolarmente elevato tra i modelli migliori.

I risultati dell'addestramento dei modelli che fanno uso della funzione prodotto ci dicono che i migliori 5 modelli hanno uno *score* medio sugli *split* che non si discosta molto dal 64,5%. I risultati sono simili a quelli precedenti, con le principali differenze legate al fatto che l'*ensemble* in questo caso è mediamente ancora più numeroso e con alberi più profondi. Il numero di colonne e di istanze del *dataset* considerate per ogni albero si mantiene alto, come è alto, ma solo in questo caso, il peso che ogni foglia dell'albero deve avere. Il parametro *gamma* mantiene dei valori relativamente ridotti come in precedenza, mentre il *learning rate* ci dice che l'addestramento mediamente è stato meno conservativo.

I risultati dell'addestramento dei modelli che fanno uso della funzione di differenza ci dicono che i migliori 5 modelli hanno uno *score* medio sugli *split* che non si discosta molto dal 66%. L'addestramento ha sempre prediletto un *ensemble* molto numeroso con alberi relativamente profondi. Il *learning rate* scelto è stato molto basso, segno che è stato preferito un approccio conservativo. L'addestramento ha preferito non scartare una percentuale di istanze del *dataset* di *training* molto elevata in corrispondenza di ogni nuovo albero, ma ha voluto trattenere molte poche colonne. Da ultimo, il valore del parametro "gamma" ha un atteggiamento un po' ondivago, anche se rimane contenuto, così come il peso minimo delle foglie dell'albero.

I risultati dell'addestramento dei modelli che fanno uso della funzione somma al quadrato ci dicono che i migliori 5 modelli hanno uno *score* medio sugli *split* che non si discosta molto dal 64%. L'addestramento ha prediletto come sempre un *ensemble* molto numeroso con alberi relativamente profondi. Il *learning rate* scelto è stato molto basso, segno che è stato preferito un approccio conservativo, così come il parametro "gamma", che nei migliori modelli è sempre stato fisso sul minimo possibile. L'addestramento ha preferito non scartare una percentuale di colonne e di istanze del *dataset* di *training* molto elevata in corrispondenza di ogni nuovo albero, segno che non è stato individuato particolare *overfitting*, anche se mediamente rispetto ai casi predenti il numero di istanze considerate per ciascun albero è stato più contenuto, sempre al di sotto dell 86%. Anche qui notiamo la mancanza di una tendenza precisa per quanto riguarda il peso minimo delle foglie.

I risultati dell'addestramento dei modelli che fanno uso della funzione differenza al quadrato ci dicono che i migliori 5 modelli hanno uno *score* medio sugli *split* che non si discosta molto dal 64%. Questo è il caso in cui l'addestramento ha prediletto un *ensemble* mediamente più numeroso possibile e tutti quanto più profondi possibile. Il *learning rate* scelto è rimasto stabilmente attorno al minimo possibile. L'addestramento ha preferito non scartare colonne del *dataset* di *training* in corrispondenza di ogni nuovo albero, ma di campionare almeno in parte le sue istanze, per controllare un potenziale *overfitting*. Da ultimo, anche il valore del parametro "gamma" rimane particolarmente ridotto tra i modelli migliori.

I risultati dell'addestramento dei modelli che fanno uso della funzione somma di quadrati ci dicono che i migliori 5 modelli hanno uno *score* medio sugli *split* che non si discosta molto dal 64%. Come in tutti i casi precedenti, l'addestramento ha prediletto un *ensemble* molto numeroso con alberi relativamente profondi. L'approccio è stato molto conservativo, con il *learning rate* fisso sul minimo possibile, così come il parametro "gamma". Questa volta, il comportamento indeciso si riscontra sia sulla percentuale di colonne da utilizzare nella costruzione di un nuovo albero, sia sul peso minimo di una foglia, che in parte anche sulla percentuale di istanze da campionare, anche se rimane sempre sopra il 56%.

Da ultimo, si è considerato l'addestramento che ha coinvolto i modelli che hanno fatto uso della funzione differenza di quadrati. In questo caso, l'addestramento sembra essere stato molto deciso: numero di alberi massimo per ogni *ensemble*, profondità massima degli stessi sempre vicina al massimo, *learning rate*, parametro "gamma" e percentuale di colonne per albero minimi. La percentuale di istanze campionate per albero rimane elevata, anche se non sempre massima, segno di un basso *overfitting*, mentre come spesso è successo il peso minimo di una foglia varia considerevolmente tra i migliori modelli.

Una volta identificato il migliore modello in corrispondenza di ciasuna funzione, ognuno di questi viene riaddestrato sull'intero *dataset* di *training* per poter valutare le sue prestazioni sul *dataset* di test. Gli *score* per ciascun modello mostrano un discreto *overfitting*, con delle differenze tra accuratezza di *training* e di *test* che oscillano attorno a 30-40 punti percentuali. La differenza minore si ha per le funzioni che impiegano la differenza, con uno scarto tra le accuratezze mai superiore al 25%. In generale comunque, i risultati sono abbastanza scoraggianti: le accuratezze sul *test set* non superano mai il 55%, segno che i modelli non sono stati in grado di generalizzare e di apprendere qualcosa di significativo dalle *feature*. Questo lo si nota anche dalle "*confusion matrix*", dove osservando i positivi e i negativi predetti, sono sempre all'incirca lo stesso numero, sia che lo fossero davvero sia che non lo fossero, con una predilezione generale per la classe dei negativi. Osservando le matrici, il modello che tendenzialmente sbaglia di meno è di nuovo quello che si basa sulla differenza, seguito dalla differenza al quadrato e dal quadrato delle differenze.

Anche in questo caso, per velocizzare il processo di addestramento, è stato utilizzato come "classificatore finale" un modello della famiglia XGBoost, lo stesso che in precedenza. Gli iperparametri da tarare per esso sono quindi gli stessi che in precedenza. Se i classificatori che forniscono il loro output sono stati determinati al passo precedente, quello finale che compone i loro risultati viene determinato tramite una seconda *grid search* con "Bayesian Optimization", che riutilizza gli stessi valori per gli iperparametri della precedente. Anche in questo caso viene fatto *caching* dell'operazione.

I risultati dell'addestramento mostrano come il modello finale risulti troppo complesso e affetto da chiaro *overfitting*, in quanto lo *score* medio tra tutti i modelli non sia mai inferiore al 99,99%. Questo si nota anche dai valori degli iperparametri per quelli che teoricamente sono i migliori modelli: in praticamente nessuno di essi si nota una chiara tendenza. La percentuale di istanze campionate, il numero di alberi, il peso minimo di una foglia, la profondità massima di un albero e la percentuale di colonne considerate per un nuovo albero possono essere alti o bassi indifferentemente rispetto allo *score* finale. Gli iperparametri che mostrano meno variabilità sono il *learning rate*, che non supera mai il 45% e il parametro "gamma".

Scelto quello che teoricamente è il miglior modello addestrato, viene poi valutato sul *test set*. Il risultato finale mostra quello che già abbiamo notato in precedenza: l'accuratezza sul *training set* è oltre il 90%, ma quella sul *test set* è attorno al 50%, segno che non ha generalizzato nulla ed ha imparato i *pattern* "a memoria". Anche in questo caso, la rete preferisce catalogare tutte le istanze come negativi, avendo una percentuale di veri positivi e veri negativi molto bassa.


